{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X & X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#from utils_3A import img_frac, find, postionning, emot_grid, data_video, flatten, cleaning, datafromto\n",
    "%matplotlib inline\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_frac(img, n, m):\n",
    "    \"\"\"\n",
    "    fractionne l'image en m lignes et n colonnes\n",
    "    \"\"\"\n",
    "    height, width=img.shape[:2]\n",
    "    x_div=[]\n",
    "    y_div=[]\n",
    "    i, j=int(width/n), int(height/m)\n",
    "    for l in range(1,n):\n",
    "        x_div.append(l*i)\n",
    "    for l in range(1,m):\n",
    "        y_div.append(l*j)\n",
    "\n",
    "    return (x_div, y_div, img)\n",
    "\n",
    "def find(x_c, x):\n",
    "    \"\"\"\n",
    "    Position de x_c dans x\n",
    "    \"\"\"\n",
    "    x=[0]+x\n",
    "    for i in range(len(x)-1):\n",
    "        if (x_c>=x[i] and x_c<x[i+1]):\n",
    "            return i\n",
    "    return i+1\n",
    "\n",
    "def postionning(frame):\n",
    "    \"\"\"\n",
    "    positionnement de chaque individu dans la grille de l'image\n",
    "    \"\"\"\n",
    "    img_path='Analyse_Json_Stade/Frames 9674/'+str(frame)+'.jpg'\n",
    "    img=cv2.imread(img_path)\n",
    "    (x_div, y_div, img)=img_frac(img, n, m)\n",
    "    base=pd.read_json('Analyse_Json_Stade/AS5I9674/frame_'+str(frame)+'.json')\n",
    "    base=base[base.type=='person']\n",
    "    pos_x=[]\n",
    "    pos_y=[]\n",
    "    for k in range(len(base)):\n",
    "        rectangle=base.iloc[k]['data']['rectangle']\n",
    "        x_c=rectangle['x']+rectangle['width']/2\n",
    "        y_c=rectangle['y']+rectangle['height']/2\n",
    "        pos_x.append(find(int(x_c), x_div))\n",
    "        pos_y.append(find(int(y_c), y_div))\n",
    "    return (pos_x, pos_y)\n",
    "\n",
    "def emot_grid(frame, m, n):\n",
    "    \"\"\"\n",
    "    Emotions dans chaque grille de l'image\n",
    "    \"\"\"\n",
    "    (pos_x, pos_y)=postionning(frame)\n",
    "    base=pd.read_json('Analyse_Json_Stade/AS5I9674/frame_'+str(frame)+'.json')\n",
    "    base=base[base.type=='person']\n",
    "    emotions=[[{'anger':0, 'fear':0, 'happiness':0, 'neutral':0, 'sadness':0, 'surprise':0} for _ in range(n)] for _ in range(m)]\n",
    "    counts=np.zeros((m,n))\n",
    "    for k in range(len(base)):\n",
    "        emot=base.iloc[k]['data']['emotion']\n",
    "        j=pos_x[k]\n",
    "        i=pos_y[k]\n",
    "        counts[i][j]+=1\n",
    "    \n",
    "        emotions[i][j]['anger']+=emot['anger']\n",
    "        emotions[i][j]['fear']+=emot['fear']\n",
    "        emotions[i][j]['happiness']+=emot['happiness']\n",
    "        emotions[i][j]['neutral']+=emot['neutral']\n",
    "        emotions[i][j]['sadness']+=emot['sadness']\n",
    "        emotions[i][j]['surprise']+=emot['surprise']\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if counts[i][j]!=0:\n",
    "                for sent in ['anger', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']:\n",
    "                    emotions[i][j][sent]/=counts[i][j]\n",
    "    return (emotions, counts)\n",
    "\n",
    "def data_video(video, n, m):\n",
    "    emotions_video=[]\n",
    "    counts_video=[]\n",
    "    i=0\n",
    "    while os.path.exists('Analyse_Json_Stade/Frames '+str(video)+'/%d.jpg' % i):   \n",
    "        (emotions, counts)=emot_grid(i, m, n)\n",
    "        emotions_video.append(emotions)\n",
    "        counts_video.append(counts)\n",
    "        i+=1\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    return (emotions_video, counts_video)\n",
    "\n",
    "def flatten(l):\n",
    "    flatten_emotions=[0 for i in range(len(l))]\n",
    "    for k in range(len(l)):\n",
    "        flatten_emotions[k]=[list(l[k][i][j].values()) for i in range(m) for j in range(n)] \n",
    "    return flatten_emotions\n",
    "\n",
    "def cleaning(l):\n",
    "    for k in range(len(l)):\n",
    "        for m in range(len(l[k])):\n",
    "            if sum(l[k][m])==0:\n",
    "                l[k][m]=[0.01, 0.01, 0.01, 0.95, 0.01, 0.01]#neutral\n",
    "    return l\n",
    "    \n",
    "def datafromto(l, fps, start, end):\n",
    "    n_i=start*fps\n",
    "    n_f=end*fps\n",
    "    x=l[n_i:n_f]\n",
    "    x=torch.tensor(np.array(x), requires_grad=False)\n",
    "    x=x.type(torch.FloatTensor)\n",
    "    return x\n",
    "\n",
    "def metric(a,b):\n",
    "    return int(a.index(min(a))==b.index(min(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "n, m=15, 10\n",
    "video='9674'\n",
    "T_enc=40\n",
    "T_dec=10\n",
    "fps=24\n",
    "\n",
    "(emotions_video, counts_video)=data_video(video, n, m)\n",
    "flatten_emotions=flatten(emotions_video)\n",
    "flatten_emotions=cleaning(flatten_emotions)\n",
    "\n",
    "tensor_enc=datafromto(flatten_emotions, fps, 0, T_enc)\n",
    "tensor_dec=datafromto(flatten_emotions, fps, T_enc, T_enc+T_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Seq2Seq Model\n",
    "\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    "<img src=https://pytorch.org/tutorials/_images/seq2seq.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2, hidden_size3):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.hidden_size3 = hidden_size3\n",
    "\n",
    "        self.ih1 = nn.Linear(input_size + hidden_size1, hidden_size1)\n",
    "        self.h1h2 = nn.Linear(hidden_size1 + hidden_size2, hidden_size2)\n",
    "        self.h2h3 = nn.Linear(hidden_size2 + hidden_size3, hidden_size3)\n",
    "        self.h3o = nn.Linear(hidden_size3, input_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden1, hidden2, hidden3):\n",
    "        combined1 = torch.cat((input, hidden1), 1)\n",
    "        hidden1 = self.ih1(combined1)\n",
    "        hidden1=F.relu(hidden1)\n",
    "        \n",
    "        combined2 = torch.cat((hidden1, hidden2), 1)\n",
    "        hidden2 = self.h1h2(combined2)\n",
    "        hidden2=F.relu(hidden2)\n",
    "        \n",
    "        combined3 = torch.cat((hidden2, hidden3), 1)\n",
    "        hidden3 = self.h2h3(combined3)\n",
    "        hidden3=F.relu(hidden3)\n",
    "        \n",
    "        output = self.h3o(hidden3)\n",
    "        output=F.tanh(output)\n",
    "        output = F.log_softmax(output)#softmax\n",
    "        return output, hidden1, hidden2, hidden3\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        #initialization of c(cell's state) et h(hidden state) h=Otanh(c)\n",
    "        return (torch.zeros(batch_size, self.hidden_size1, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_size2, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_size3, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size=6\n",
    "hidden_size1=10\n",
    "hidden_size2=20\n",
    "hidden_size3=10\n",
    "\n",
    "encoder=EncoderRNN(input_size, hidden_size1,hidden_size2, hidden_size3)\n",
    "X_enc=tensor_enc[0]\n",
    "\n",
    "batch_size=X_enc.shape[0]\n",
    "hidden1_enc, hidden2_enc, hidden3_enc= encoder.initHidden(batch_size)\n",
    "output_enc, hidden1_enc, hidden2_enc, hidden3_enc=encoder(X_enc, hidden1_enc, hidden2_enc, hidden3_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_enc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input and hidden state, \n",
    "and the first hidden state is the context vector (the encoder's\n",
    "last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.hidden_size3 = hidden_size3\n",
    "\n",
    "        self.ih1 = nn.Linear(output_size + hidden_size1, hidden_size1)\n",
    "        self.h1h2 = nn.Linear(hidden_size1 + hidden_size2, hidden_size2)\n",
    "        self.h2h3 = nn.Linear(hidden_size2 + hidden_size3, hidden_size3)\n",
    "        self.h3o = nn.Linear(hidden_size3, output_size)\n",
    "\n",
    "    def forward(self, input, hidden1, hidden2, hidden3):\n",
    "        combined1 = torch.cat((input, hidden1), 1)\n",
    "        hidden1 = self.ih1(combined1)\n",
    "        hidden1=F.relu(hidden1)\n",
    "        \n",
    "        combined2 = torch.cat((hidden1, hidden2), 1)\n",
    "        hidden2 = self.h1h2(combined2)\n",
    "        hidden2=F.relu(hidden2)\n",
    "        \n",
    "        combined3 = torch.cat((hidden2, hidden3), 1)\n",
    "        hidden3 = self.h2h3(combined3)\n",
    "        hidden3=F.relu(hidden3)\n",
    "        \n",
    "        output = self.h3o(hidden3)\n",
    "        output=F.tanh(output)\n",
    "        output = F.log_softmax(output)#softmax\n",
    "        return output, hidden1, hidden2, hidden3\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        #initialization of c(cell's state) et h(hidden state) h=Otanh(c)\n",
    "        return (torch.zeros(batch_size, self.hidden_size1, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_size2, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_size3, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size=6\n",
    "hidden_size1=10\n",
    "hidden_size2=20\n",
    "hidden_size3=10\n",
    "output_size=input_size\n",
    "\n",
    "encoder=EncoderRNN(input_size, hidden_size1, hidden_size2, hidden_size3)\n",
    "X_enc=tensor_enc[0]\n",
    "\n",
    "batch_size=X_enc.shape[0]\n",
    "hidden1_enc, hidden2_enc, hidden3_enc= encoder.initHidden(batch_size)\n",
    "output_enc, hidden1_enc, hidden2_enc, hidden3_enc=encoder(X_enc, hidden1_enc, hidden2_enc, hidden3_enc)\n",
    "\n",
    "decoder=DecoderRNN(hidden_size1, hidden_size2, hidden_size3, output_size)\n",
    "X_dec=tensor_dec[0]\n",
    "\n",
    "hidden1_dec, hidden2_dec, hidden3_dec=hidden1_enc, hidden2_enc, hidden3_enc\n",
    "output_dec, hidden1_dec, hidden2_dec, hidden3_dec=decoder(X_dec, hidden1_dec, hidden2_dec, hidden3_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_dec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train we run the input through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    batch_size=input_tensor.shape[1]\n",
    "    hidden1_enc, hidden2_enc, hidden3_enc= encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.shape[0]\n",
    "    target_length = target_tensor.shape[0]\n",
    "\n",
    "    loss = 0\n",
    "    metrics=0\n",
    "    for ei in range(input_length):\n",
    "        output_enc, hidden1_enc, hidden2_enc, hidden3_enc=encoder(input_tensor[ei], hidden1_enc, hidden2_enc, hidden3_enc)\n",
    "\n",
    "    hidden1_dec, hidden2_dec, hidden3_dec=hidden1_enc, hidden2_enc, hidden3_enc\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for di in range(target_length):\n",
    "        output_dec, hidden1_dec, hidden2_dec, hidden3_dec=decoder(target_tensor[di], hidden1_dec, hidden2_dec, hidden3_dec)\n",
    "        loss += criterion(output_dec, target_tensor[di])\n",
    "        a=[metric(list(output_dec[k]), list(target_tensor[di][k])) for k in range(output_dec.shape[0])]\n",
    "        metrics+=sum(a)/output_dec.shape[0]\n",
    "        \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    #return loss.item()/target_length\n",
    "    return (loss.item()/target_length, metrics/target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    \"\"\"\n",
    "    Transforme les secondes en minutes et secondes\n",
    "    \"\"\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(input_tensor, target_tensor, encoder, decoder, n_iters, print_every, learning_rate):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    print_metrics_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    batch_size=input_tensor.shape[0]\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        #penser à la permutation\n",
    "        #loss= train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        loss, metrics = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        print_metrics_total += metrics\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_metrics_avg = print_metrics_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_metrics_total = 0\n",
    "            \n",
    "            print('# %s (%d %d%%) (%.4f %.2f%%)' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg, print_metrics_avg*100))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these helper functions in place, we can actually initialize a network and start training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=6\n",
    "hidden_size1=10\n",
    "hidden_size2=20\n",
    "hidden_size3=10\n",
    "output_size=input_size\n",
    "\n",
    "n_iters=20000\n",
    "print_every=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "input_tensor=tensor_enc\n",
    "target_tensor=torch.tensor(np.log(np.array(tensor_dec)), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "encoder=EncoderRNN(input_size, hidden_size1, hidden_size2, hidden_size3).to(device)\n",
    "decoder=DecoderRNN(hidden_size1, hidden_size2, hidden_size3, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 5m 48s (- 52m 19s) (100 10%) (0.4958 43.87%)\n",
      "# 11m 49s (- 47m 17s) (200 20%) (0.2973 50.49%)\n",
      "# 18m 10s (- 42m 25s) (300 30%) (0.2626 58.65%)\n",
      "# 23m 45s (- 35m 38s) (400 40%) (0.2581 58.93%)\n",
      "# 29m 20s (- 29m 20s) (500 50%) (0.2564 59.85%)\n",
      "# 35m 23s (- 23m 35s) (600 60%) (0.2541 60.14%)\n",
      "# 41m 29s (- 17m 47s) (700 70%) (0.2546 59.72%)\n",
      "# 48m 5s (- 12m 1s) (800 80%) (0.2539 60.75%)\n",
      "# 54m 26s (- 6m 2s) (900 90%) (0.2511 62.66%)\n",
      "# 60m 36s (- 0m 0s) (1000 100%) (0.2482 62.75%)\n"
     ]
    }
   ],
   "source": [
    "trainIters(input_tensor, target_tensor, encoder, decoder, n_iters, print_every, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
