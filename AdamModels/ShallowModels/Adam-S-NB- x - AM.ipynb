{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X & AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#from utils_3A import img_frac, find, postionning, emot_grid, data_video, flatten, cleaning, datafromto\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_frac(img, n, m):\n",
    "    \"\"\"\n",
    "    fractionne l'image en m lignes et n colonnes\n",
    "    \"\"\"\n",
    "    height, width=img.shape[:2]\n",
    "    x_div=[]\n",
    "    y_div=[]\n",
    "    i, j=int(width/n), int(height/m)\n",
    "    for l in range(1,n):\n",
    "        x_div.append(l*i)\n",
    "    for l in range(1,m):\n",
    "        y_div.append(l*j)\n",
    "\n",
    "    return (x_div, y_div, img)\n",
    "\n",
    "def find(x_c, x):\n",
    "    \"\"\"\n",
    "    Position de x_c dans x\n",
    "    \"\"\"\n",
    "    x=[0]+x\n",
    "    for i in range(len(x)-1):\n",
    "        if (x_c>=x[i] and x_c<x[i+1]):\n",
    "            return i\n",
    "    return i+1\n",
    "\n",
    "def postionning(frame):\n",
    "    \"\"\"\n",
    "    positionnement de chaque individu dans la grille de l'image\n",
    "    \"\"\"\n",
    "    img_path='Analyse_Json_Stade/Frames 9674/'+str(frame)+'.jpg'\n",
    "    img=cv2.imread(img_path)\n",
    "    (x_div, y_div, img)=img_frac(img, n, m)\n",
    "    base=pd.read_json('Analyse_Json_Stade/AS5I9674/frame_'+str(frame)+'.json')\n",
    "    base=base[base.type=='person']\n",
    "    pos_x=[]\n",
    "    pos_y=[]\n",
    "    for k in range(len(base)):\n",
    "        rectangle=base.iloc[k]['data']['rectangle']\n",
    "        x_c=rectangle['x']+rectangle['width']/2\n",
    "        y_c=rectangle['y']+rectangle['height']/2\n",
    "        pos_x.append(find(int(x_c), x_div))\n",
    "        pos_y.append(find(int(y_c), y_div))\n",
    "    return (pos_x, pos_y)\n",
    "\n",
    "def emot_grid(frame, m, n):\n",
    "    \"\"\"\n",
    "    Emotions dans chaque grille de l'image\n",
    "    \"\"\"\n",
    "    (pos_x, pos_y)=postionning(frame)\n",
    "    base=pd.read_json('Analyse_Json_Stade/AS5I9674/frame_'+str(frame)+'.json')\n",
    "    base=base[base.type=='person']\n",
    "    emotions=[[{'anger':0, 'fear':0, 'happiness':0, 'neutral':0, 'sadness':0, 'surprise':0} for _ in range(n)] for _ in range(m)]\n",
    "    counts=np.zeros((m,n))\n",
    "    for k in range(len(base)):\n",
    "        emot=base.iloc[k]['data']['emotion']\n",
    "        j=pos_x[k]\n",
    "        i=pos_y[k]\n",
    "        counts[i][j]+=1\n",
    "    \n",
    "        emotions[i][j]['anger']+=emot['anger']\n",
    "        emotions[i][j]['fear']+=emot['fear']\n",
    "        emotions[i][j]['happiness']+=emot['happiness']\n",
    "        emotions[i][j]['neutral']+=emot['neutral']\n",
    "        emotions[i][j]['sadness']+=emot['sadness']\n",
    "        emotions[i][j]['surprise']+=emot['surprise']\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if counts[i][j]!=0:\n",
    "                for sent in ['anger', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']:\n",
    "                    emotions[i][j][sent]/=counts[i][j]\n",
    "    return (emotions, counts)\n",
    "\n",
    "def data_video(video, n, m):\n",
    "    emotions_video=[]\n",
    "    counts_video=[]\n",
    "    i=0\n",
    "    while os.path.exists('Analyse_Json_Stade/Frames '+str(video)+'/%d.jpg' % i):   \n",
    "        (emotions, counts)=emot_grid(i, m, n)\n",
    "        emotions_video.append(emotions)\n",
    "        counts_video.append(counts)\n",
    "        i+=1\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    return (emotions_video, counts_video)\n",
    "\n",
    "def flatten(l):\n",
    "    flatten_emotions=[0 for i in range(len(l))]\n",
    "    for k in range(len(l)):\n",
    "        flatten_emotions[k]=[list(l[k][i][j].values()) for i in range(m) for j in range(n)] \n",
    "    return flatten_emotions\n",
    "\n",
    "def cleaning(l):\n",
    "    for k in range(len(l)):\n",
    "        for m in range(len(l[k])):\n",
    "            if sum(l[k][m])==0:\n",
    "                l[k][m]=[0.01, 0.01, 0.01, 0.95, 0.01, 0.01]#neutral\n",
    "    return l\n",
    "    \n",
    "def datafromto(l, fps, start, end):\n",
    "    n_i=start*fps\n",
    "    n_f=end*fps\n",
    "    x=l[n_i:n_f]\n",
    "    x=torch.tensor(np.array(x), requires_grad=False)\n",
    "    x=x.type(torch.FloatTensor)\n",
    "    return x\n",
    "\n",
    "def metric(a,b):\n",
    "    return int(a.index(min(a))==b.index(min(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "n, m=15, 10\n",
    "video='9674'\n",
    "T_enc=40\n",
    "T_dec=10\n",
    "fps=24\n",
    "\n",
    "(emotions_video, counts_video)=data_video(video, n, m)\n",
    "flatten_emotions=flatten(emotions_video)\n",
    "flatten_emotions=cleaning(flatten_emotions)\n",
    "\n",
    "tensor_enc=datafromto(flatten_emotions, fps, 0, T_enc)\n",
    "tensor_dec=datafromto(flatten_emotions, fps, T_enc, T_enc+T_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([960, 150, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Seq2Seq Model\n",
    "\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    "<img src=https://pytorch.org/tutorials/_images/seq2seq.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden=F.relu(hidden)\n",
    "        \n",
    "        output = self.h2o(hidden)\n",
    "        output=F.tanh(output)\n",
    "        output = F.log_softmax(output)#softmax\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        #initialization of c(cell's state) et h(hidden state) h=Otanh(c)\n",
    "        return torch.zeros(batch_size, self.hidden_size, device=device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size=6\n",
    "hidden_size=10\n",
    "\n",
    "encoder=EncoderRNN(input_size, hidden_size)\n",
    "X_enc=tensor_enc[0]\n",
    "\n",
    "batch_size=X_enc.shape[0]\n",
    "hidden_enc= encoder.initHidden(batch_size)\n",
    "output_enc, hidden_enc=encoder(X_enc, hidden_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_enc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder=EncoderRNN(input_size, hidden_size)\n",
    "hidden_enc= encoder.initHidden(batch_size)\n",
    "input_length = tensor_enc.shape[0]\n",
    "encoder_outputs=[]\n",
    "for ei in range(input_length):\n",
    "    output_enc, hidden_enc= encoder(tensor_enc[ei], hidden_enc)\n",
    "    encoder_outputs.append(output_enc)\n",
    "#encoder_outputs=torch.cat(encoder_outputs,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of values.\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input and hidden state, \n",
    "and the first hidden state is the context vector (the encoder's\n",
    "last hidden state)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    "<img src=https://i.imgur.com/1152PYf.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.attn = nn.Linear(output_size + hidden_size, 1)\n",
    "        self.attn_combine = nn.Linear(self.output_size*2, self.output_size)\n",
    "        \n",
    "        self.i2h = nn.Linear(output_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, encoder_outputs, hidden):\n",
    "        attns=[]\n",
    "        for k in range(len(encoder_outputs)):\n",
    "            x=torch.cat((encoder_outputs[k],hidden), dim=1) #[batch_size, input_size+hidden_size]\n",
    "            x=self.attn(x) #[batch_size,1]\n",
    "            attns.append(x)\n",
    "            \n",
    "        attns=torch.transpose(torch.cat(tuple(attns),1),0,1) #[input_lenth, batch_size,1]    \n",
    "        attn_weights = torch.transpose(F.softmax(attns, dim=0),0,1) #[input_lenth, batch_size] \n",
    "        attn_applied=[]\n",
    "        for k in range(encoder_outputs.shape[1]):\n",
    "            x=torch.matmul(torch.transpose(torch.transpose(encoder_outputs, 0, 1)[k],0,1), \n",
    "                   attn_weights[k].view(attn_weights[k].shape[0],1))\n",
    "            attn_applied.append(x)\n",
    "        attn_applied=torch.transpose(torch.cat(tuple(attn_applied),1),0,1) #[batch_size, input_size]\n",
    "\n",
    "        ci = torch.cat((input, attn_applied), 1) #[batch_size, 2*input_size]\n",
    "        ci = self.attn_combine(ci) #[batch_size, input_size]\n",
    "        ci = F.relu(ci)\n",
    "       \n",
    "        combined = torch.cat((ci, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden=F.relu(hidden)\n",
    "        \n",
    "        output = self.h2o(hidden)\n",
    "        output=F.tanh(output)\n",
    "        output = F.log_softmax(output)#softmax\n",
    "        return output, attn_weights, hidden    \n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test decodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size=6\n",
    "hidden_size=10\n",
    "output_size=input_size\n",
    "encoder=EncoderRNN(input_size, hidden_size)\n",
    "X_enc=tensor_enc[0]\n",
    "\n",
    "batch_size=X_enc.shape[0]\n",
    "hidden_enc= encoder.initHidden(batch_size)\n",
    "output_enc, hidden_enc=encoder(X_enc, hidden_enc)\n",
    "\n",
    "hidden_dec=hidden_enc\n",
    "encoder_outputs=tensor_enc\n",
    "X_dec=tensor_dec[0]\n",
    "decoder=AttnDecoderRNN(hidden_size, output_size)\n",
    "\n",
    "output_dec, attn_weights, hidden=decoder(X_dec,encoder_outputs, hidden_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attn_weights[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train we run the input through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    batch_size=input_tensor.shape[1]\n",
    "    hidden_enc= encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.shape[0]\n",
    "    target_length = target_tensor.shape[0]\n",
    "    \n",
    "    #Setting new minibatchs\n",
    "    minibatch_enc=random.randint(10,20)*24\n",
    "    minibatch_dec=random.randint(5,10)*24\n",
    "    \n",
    "    input_tensor=input_tensor[input_length-minibatch_enc:input_length]\n",
    "    target_tensor=target_tensor[:minibatch_dec]\n",
    "    \n",
    "    input_length = input_tensor.shape[0]\n",
    "    target_length = target_tensor.shape[0]\n",
    "    \n",
    "    loss = 0\n",
    "    metrics=0\n",
    "    encoder_outputs=[]\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        output_enc, hidden_enc= encoder(input_tensor[ei], hidden_enc)\n",
    "        encoder_outputs.append(output_enc)\n",
    "    encoder_outputs=torch.cat(tuple(encoder_outputs),0).view(input_tensor.shape[0],input_tensor.shape[1],input_tensor.shape[2])    \n",
    "    hidden_dec=hidden_enc\n",
    "    attns=[]\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        output_dec, attn_weights, hidden_dec= decoder(target_tensor[di], encoder_outputs, hidden_dec)# Teacher forcing\n",
    "        loss += criterion(output_dec, target_tensor[di])\n",
    "        attns.append(attn_weights)\n",
    "        a=[metric(list(output_dec[k]), list(target_tensor[di][k])) for k in range(output_dec.shape[0])]\n",
    "        metrics+=sum(a)/output_dec.shape[0]\n",
    "        \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return (loss.item()/target_length, metrics/target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_enc=random.randint(10,20)*24\n",
    "minibatch_dec=random.randint(5,10)*24\n",
    "a=tensor_enc.shape[0]\n",
    "new=tensor_enc[a-minibatch_enc:a]\n",
    "new1=tensor_dec[:minibatch_dec]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    \"\"\"\n",
    "    Transforme les secondes en minutes et secondes\n",
    "    \"\"\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(input_tensor, target_tensor, encoder, decoder, n_iters, print_every, learning_rate):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    print_metrics_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    batch_size=input_tensor.shape[0]\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        #penser à la permutation\n",
    "        loss, metrics= train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        print_metrics_total += metrics\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_metrics_avg = print_metrics_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_metrics_total = 0\n",
    "            \n",
    "            print('# %s (%d %d%%) (%.4f %.2f%%)' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg, print_metrics_avg*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these helper functions in place, we can actually initialize a network and start training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=6\n",
    "hidden_size=20\n",
    "output_size=input_size\n",
    "\n",
    "n_iters=20000\n",
    "print_every=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "input_tensor=tensor_enc\n",
    "target_tensor=torch.tensor(np.log(np.array(tensor_dec)), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "encoder=EncoderRNN(input_size, hidden_size).to(device)\n",
    "decoder=AttnDecoderRNN(hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "trainIters(input_tensor, target_tensor, encoder, decoder, n_iters, print_every, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
